{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import warnings, re\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=pd.read_csv('/kaggle/input/fake-news/train.csv',index_col='id')\ntest_data=pd.read_csv('/kaggle/input/fake-news/test.csv',index_col='id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking if there are overlapping missing values from **all** arrtibutes of a given **entry**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aparently there's no missing values from the labels, but there are some on the attributes,<br>\nfollowing is to remove and check overlapping missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.dropna(how='all').isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same is applicable for the **Test Data** too"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_data.isna().sum(),'\\n')\nprint(test_data.dropna(how='all').isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handeling Missing Values\n\n> **id**: *No NaNs* <br>\n> **title**: *'Not Mentioned' to replace NaN*<br>\n> **author**: *'Not Mentioned' to replace NaN*<br>\n> **text**: *'Not Mentioned' to replace NaN*<br>\n> **label**: *No NaN*<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.fillna('Not Mentioned',inplace=True)\ntest_data.fillna('Not Mentioned',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the data consists of **Author** to a certain article, "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['author'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of fake labels and  no. real labels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_data.loc[train_data['label']==1]), len(train_data.loc[train_data['label']==0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its almost a 50-50 split, and we can say the training data is balanced"},{"metadata":{},"cell_type":"markdown","source":"### Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_cleaning(text):\n    \"\"\"\n    Removing all characters except alphabets\n    \"\"\"\n    text = re.sub(r'[^a-z]', ' ', text.lower())\n    return text\n\ntrain_data['text']=train_data['text'].apply(text_cleaning)\ntrain_data['title']=train_data['title'].apply(text_cleaning)\ntrain_data['author']=train_data['author'].apply(text_cleaning)\n\n#applying the same preprocessing for test data\ntest_data['text']=test_data['text'].apply(text_cleaning)\ntest_data['title']=test_data['title'].apply(text_cleaning)\ntest_data['author']=test_data['author'].apply(text_cleaning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Ingestion Pipe using TorchText"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_data, val_data=train_test_split(train_data, test_size=0.1, shuffle=True)\nlen(train_data), len(val_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !mkdir cache\n\n# train_data.to_csv('cache/train.csv', index=False)\n# val_data.to_csv('cache/val.csv', index=False)\n# test_data.to_csv('cache/test.csv', index=False)\n\n# del val_data, test_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word Embeddings\nThis ingestion pipeline is for using **Word Embeddings**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nfrom nltk.corpus import stopwords\n\nspacy_nlp=spacy.load('en')\nstopword_list=stopwords.words('english')\n\ndef tokenization(text, MAX_LEN=20000):\n    text=re.sub(' +', ' ',\n                re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\",\n                       ' ',text))\n    text=text if len(text)<=MAX_LEN else text[:MAX_LEN]\n    return [x.text for x in spacy_nlp.tokenizer(text) if (x.text!=' ') and (x.text not in stopword_list)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchtext.vocab import Vectors, Vocab\nfrom collections import Counter\n\n\ngloveVectors=Vectors(name='../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt')\n\ncounter = Counter()\nfor i in tqdm(train_data.index):\n    counter.update(tokenization(train_data['text'][i]+' '+train_data['title'][i]+' '+train_data['author'][i]))\n#     counter.update(i.text+i.keyword+i.location)\n    \nvocabulary=Vocab(counter, max_size=20000, min_freq=4, vectors=gloveVectors, specials=['<pad>', '<unk>'])\n\nprint('Embedding vocab size: ', vocabulary.vectors.size(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch, torchtext, warnings\nfrom torchtext.data import Field, LabelField, Dataset, Example, TabularDataset, BucketIterator\n\nwarnings.filterwarnings(\"ignore\")\n\n\nclass NewsDataset(Dataset):\n    def __init__(self, df, fields, **kwargs):\n        examples=[]\n        for i, row in df.iterrows():\n            examples.append(Example.fromlist([row.title, row.author, row.text, row.label],\n                                                           fields))\n        #print(i,row.author)\n        super().__init__(examples, fields, **kwargs)\n        \n    @staticmethod\n    def sort_key(x):\n        return len(x.text)\n    \n    @classmethod\n    def splits(cls, fields, train_df=None, val_df=None, **kwargs):\n        train_data, val_data=(None, None)\n        \n        if train_df is not None:\n            train_data=cls(train_df.copy(), fields, **kwargs)\n            \n        if val_df is not None:\n            val_data=cls(val_df.copy(), fields, **kwargs)\n            \n        return tuple(d for d in (train_data, val_data) if d is not None)\n    \n\nText=Field(tokenization, include_lengths=True)\nTitle=Field(tokenization, include_lengths=True)\nAuthor=Field(tokenization, include_lengths=True)\nLabel=LabelField(dtype=torch.float)\n\nText.vocab=vocabulary\nTitle.vocab=vocabulary\nAuthor.vocab=vocabulary\n\nfields= [('title', Title),('author', Author), ('text', Text),('label', Label)]\n\ntrain_ds, val_ds= NewsDataset.splits(fields, train_df=train_data, val_df=val_data)\n\n# train_data, val_data= TabularDataset.splits(path='cache',\n#                                            train='train.csv',\n#                                            validation='val.csv',\n#                                            skip_header=True,\n#                                            format='csv',\n#                                            fields=fields)\n\nLabel.build_vocab(train_ds)\n\n\ndel train_data, val_data\n#sampling random example\n#print(vars(train_ds[61]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(vars(train_data.examples[0]))\nprint(f'Number of training examples: {len(train_ds)}')\nprint(f'Number of validation examples: {len(val_ds)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Patching missing **text** in process of the tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, elem in enumerate(train_ds):\n    bucket=vars(elem)\n    if len(bucket['text'])<1:\n#         print(bucket)\n        \n        if len(bucket['title'])>0:\n            train_ds[i].text=train_ds[i].title\n        else:\n            train_ds[i].text=['not','mentioned']\n        print(vars(train_ds[i]))\n        \nfor i, elem in enumerate(val_ds):\n    bucket=vars(elem)\n    if len(bucket['text'])<1:\n#         print(bucket)\n        \n        if len(bucket['title'])>0:\n            val_ds[i].text=val_ds[i].title\n        else:\n            val_ds[i].text=['not','mentioned']\n        print(vars(val_ds[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=64\ndevice=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iterator, valid_iterator= BucketIterator.splits(\n                                (train_ds, val_ds),\n                                batch_size=batch_size,\n                                sort_within_batch=True,\n                                device=device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Sanity check for zero-len texts"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in train_iterator:\n    if 0 in i.text[1]:\n        print(i.text[1], i.text[0])\n\nfor i in valid_iterator:\n    if 0 in i.text[1]:\n        print(i.text[1], i.text[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Simple Single channel--text biLSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\nlr=0.001\n\ninput_dims=len(vocabulary)\nembedding_dims=100\nhidden_dims=128\noutput_dims=1\nn_layers=2\nbidirectional=True\ndropoutP=0.2\n\npad_idx=0\n\ndef bin_accuracy(preds, y):\n    correct=(torch.round(torch.sigmoid(preds))==y).float()\n    return correct.sum()/len(correct)\n\nclass biLSTM_single(nn.Module):\n    def __init__(self, vocab_size, embedding_dims, hidden_dims,\n              n_layers,bidirectional, dropoutP, output_dims, pad_idx):\n        \n        super().__init__()\n        self.embeddings=nn.Embedding(vocab_size, embedding_dims, padding_idx=pad_idx)\n        \n        self.lstm=nn.LSTM(embedding_dims, hidden_dims, \n                          num_layers=n_layers, bidirectional=bidirectional,\n                          dropout=dropoutP)\n        \n        self.fc1=nn.Linear(hidden_dims*2, hidden_dims)\n        self.fc2=nn.Linear(hidden_dims, output_dims)\n        self.drop=nn.Dropout(dropoutP)\n        \n    def forward(self, text, text_len):\n        #print(text.shape)\n        #[seq_len, batch_size]-->[seq_len, batch_size, embedding_dims]\n        embedding=self.embeddings(text) \n        #print(embedding.shape)\n        #[seq_len, batch_size, embedding_dims] -> [seq_len*batch_size, embedding_dims]\n        packed_embeddings=nn.utils.rnn.pack_padded_sequence(embedding, text_len)\n        #hidden:[num_layers * num_dir, batch_size, hidden_dims]\n        packed_out, (hidden, cell_state)=self.lstm(packed_embeddings)\n        #print(hidden.shape)\n        #[num_layers * num_dir, batch_size, hidden_dims] -> [batch_size, hidden_dims*2]\n        hidden=self.drop(torch.cat((hidden[-2,:,:],hidden[-1,:,:]), dim=1))\n        #print(hidden.shape)\n        #[batch_size, hidden_dims*2] --> [batch_size, hidden_dims]\n        output=self.drop(self.fc1(hidden))\n        #print(output.shape)\n        #[batch_size, hidden_dims]->[batch_size, out_dims]\n        output=self.fc2(output)\n        #print(output.shape)\n        return output\n    \nsingleChannelBiLSTM=biLSTM_single(input_dims, embedding_dims, hidden_dims,\n                                  n_layers, bidirectional, dropoutP, output_dims, pad_idx)\n\nsingleChannelBiLSTM.embeddings.weight.data.copy_(vocabulary.vectors)\n\nsingleChannelBiLSTM.to(device)\n\ncriterion=nn.BCEWithLogitsLoss()\noptimizer=torch.optim.Adam(singleChannelBiLSTM.parameters(), lr=lr)\nprint(sum(p.numel() for p in singleChannelBiLSTM.parameters() if p.requires_grad),' trainable prams')\n\n#loading from previous\nsingleChannelBiLSTM.load_state_dict(torch.load('../input/fakenewsmodelweights/singleBiLstm-bestLoss.pt',map_location='cpu'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, iterator):\n    epoch_loss, epoch_acc = 0, 0\n    \n    model.train()\n    \n    for batch in tqdm(iterator, total=len(iterator)):\n        text, text_len = batch.text\n        #print(text_len)\n        optimizer.zero_grad()\n        preds=model(text, text_len).squeeze(1)\n        #print(preds)\n        loss=criterion(preds, batch.label)\n        acc=bin_accuracy(preds, batch.label)\n        \n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss+=loss.item()\n        epoch_acc+=acc.item()\n        #print(epoch_loss, epoch_acc)\n    return (epoch_loss/len(iterator), epoch_acc/len(iterator))\n\ndef evaluate(model, iterator):\n    epoch_loss, epoch_acc = 0, 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n        for batch in tqdm(iterator, total=len(iterator)):\n            text, text_len = batch.text\n\n            preds=model(text, text_len).squeeze(1)\n            #print(preds)\n            loss=criterion(preds, batch.label)\n            acc=bin_accuracy(preds, batch.label)\n            \n            epoch_acc+=acc.item()\n            epoch_loss+=loss.item()\n            #print(epoch_acc)\n    return (epoch_loss/len(iterator),epoch_acc/len(iterator))\n\nfrom time import time\n\nstartT=time()\n\nloss, val_loss, acc, val_acc=[], [], [], []\n\nEPOCHS=10\n\nbest_loss=999\n\nfor epoch in range(EPOCHS):\n    train_loss, train_acc= train(singleChannelBiLSTM, train_iterator)\n    print(f'Epoch {epoch}/{EPOCHS}\\nTraining: Loss {train_loss} Accuracy {train_acc}')\n    valid_loss, valid_acc= evaluate(singleChannelBiLSTM, valid_iterator)\n    print(f'Epoch {epoch}/{EPOCHS}\\nValidation: Loss {valid_loss} Accuracy {valid_acc}')\n    \n    loss.append(train_loss)\n    acc.append(train_acc)\n    \n    val_acc.append(valid_acc)\n    val_loss.append(valid_loss)\n\n    if valid_loss<best_loss:\n        best_loss=valid_loss\n        torch.save(singleChannelBiLSTM.state_dict(),'singleBiLstm-bestLoss.pt')\n        \nprint(time()-startT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(singleChannelBiLSTM.state_dict(),'singleBiLstmv2.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(EPOCHS),loss)\nplt.plot(range(EPOCHS),acc)\nplt.plot(range(EPOCHS),val_loss)\nplt.plot(range(EPOCHS),val_acc)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(EPOCHS),loss)\nplt.plot(range(EPOCHS),acc)\nplt.plot(range(EPOCHS),val_loss)\nplt.plot(range(EPOCHS),val_acc)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\n\nwith open('history.json','w') as fp:\n    json.dump({'train loss':loss,\n               'train accuracy':acc,\n                'train loss':val_loss,\n               'train accuracy':val_acc,})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate"},{"metadata":{"trusted":true},"cell_type":"code","source":"#singleChannelBiLSTM.load_state_dict(torch.load('../input/fakenewsmodelweights/singleBiLstm-bestLoss.pt',map_location='cpu'))\n# singleChannelBiLSTM.eval()\nsingleChannelBiLSTM.to(torch.device('cpu'))\ndef infer(text, author=None, title=None, preprocessed=False):\n    singleChannelBiLSTM.eval()\n    \n    if not preprocessed:\n        text_arr=[vocabulary.stoi[token] for token in tokenization(text)]\n    else:\n        text_arr=[vocabulary.stoi[token] for token in text]\n    \n    if len(text_arr):\n        with torch.no_grad():\n            text=torch.LongTensor([text_arr]).view(-1,1)\n            text_len=torch.LongTensor([text.shape[1]])\n            return int(torch.round(torch.sigmoid(singleChannelBiLSTM(text, text_len).squeeze(1))).item())\n    else:\n        return 0\n\n# test_preds=[]\n# for i in tqdm(test_data.iterrows(), total=len(test_data)):\n#     test_preds.append(infer(i[1]['text']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\n\ntrue=[label for label in val_ds.label]\npredicted=[infer(val.text, preprocessed=True) for val in val_ds]\n\n\nprint(confusion_matrix(true , predicted),'\\n\\n\\n')\nprint(classification_report(true,predicted,target_names=['real','fake']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submissions=pd.DataFrame({'id':test_data.index.values,'label':test_preds})\nmy_submissions.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}